
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="EN" lang="EN" dir="ltr">
<head profile="http://gmpg.org/xfn/11">
<title>CS592</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<meta http-equiv="imagetoolbar" content="no" />
<link rel="stylesheet" href="styles/layout.css" type="text/css" />
<script type="text/javascript" src="scripts/mootools.js"></script>
</head>
<body id="top">
<!-- ####################################################################################################### -->
<div class="wrapper col2">
  <div id="header">
    <div id="logo">
      <h1><a href="#">CS 59200 - HAI</a></h1>
      <h2>Human-AI Interaction (Spring 2024)</h2>
    </div>
    <div id="topnav">
      <ul>
        <li><a href="index.html">Home</a><br><span>is this course for you?</span></li>
        <li class="active"><a href="calendar.html">Calendar</a><span>slides, due dates, etc</span></li>
        <li><a href="syllabus.html">Syllabus</a><span>About this Course</span></li>
        <li class="last"><a href="readings.html">Resources</a><span>tips, books, articles</span></li>
      </ul>
    </div>
    <br class="clear" />
  </div>
</div>
<!-- ####################################################################################################### -->

<!-- ####################################################################################################### -->
<div class="wrapper col4">
  <div id="container">

  	<!--Presentation <a href="https://doodle.com/poll/eizys9afwrd7d58u">Schedule</a>.
  	<br>Paper Critique Forms: <a href="critique/PaperCritique.pdf">PDF</a> , <a href="critique/PaperCritique.docx">Word</a>-->
    The following schedule is tentative and subject to change. 
   
    <div>
      <br>
      <table>
        <tr class='heading'>
          <td width="45"><b>Date</b></td>
          <td width="180"><b>Topic</b></td>
          <td width="500"><b>Readings</b></td>
          <td><b>Assignments & Project</b></td>
        </tr>
        <tr>
          <td>Jan 8</td>
          <td>Introduction & Course overview <!--<br><br> <i>Lecture</i> (<a href="files/lecture1_0820.pdf">slides</a>)--></td>
          <td></td>
          <td></td>
        </tr>
        <tr class='light'>
          <td>Jan 15</td>
          <td class='dark'><i>No class (Martin Luther King Day)</i></td>
          <td>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Jan 22</td>
          <td>Fundamentals of Human Cognition and Artificial Intelligence<br><br> <i>Lecture</i></td>
          <td>
            <p><span class='optional'>Optional</span></p>
            <p>Evans, <a href="https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8295.1984.tb01915.x?casa_token=6z11qEsiiEwAAAAA%3AnHXC21QOxp2Ms4tWxr82EVibPxsCz6To_U8EAq_eGrhcYTKl0d5_B65Ugh8Mv7yuquDn8iu6gAdLOr-0">Heuristic and Analytic Processes in Reasoning</a>. British Journal of Psychology 1984</p>
            <p>Kahneman and Tversky, <a href="https://www.science.org/doi/abs/10.1126/science.185.4157.1124?casa_token=PkTB-CIZIfQAAAAA:Ic4KtNauoiJ1eL7Nb4AkUyitGU4pcm0tcPP60jdo3bqSNUS19V4XFZ4WHo1VmOql_I1cAYxjj0ooo4Fu">Judgment under Uncertainty: Heuristics and Biases</a>. Science 1974</p>
            <p><a href="https://www.sciencedirect.com/science/article/abs/pii/B9780444818621500698">Chapter 3: Mental Models and User Models</a>. Handbook of Human-Computer Interaction</p>
            <p>Russell and Norvig. <a href="https://aima.cs.berkeley.edu/">Artificial Intelligence: A Modern Approach, 4th Edition</a>. 2020</p>
          </td>
          <td></td>
          <!--<td>Crowd workers <br><br> <i>Lecture</i> (<a href="files/lecture3_0827.pdf">slides</a>)</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Martin et al. <a href="https://dl.acm.org/citation.cfm?id=2531663">Being a Turker</a>. CSCW'14</p>
            <p>Yin et al. <a href="https://dl.acm.org/citation.cfm?id=2883036">The Communication Network Within the Crowd</a>. WWW'16</p>
            <p><span class='optional'>Optional</span></p>
            <p>Gray et al. <a href="https://dl.acm.org/citation.cfm?id=2819942">The Crowd is a Collaborative Network</a>. CSCW'16</p>
            <p>Difallah et al. <a href="https://dl.acm.org/citation.cfm?id=3159661">Demographics and Dynamics of Mechanical Turk Workers</a>. WSDM'18<p>
          </td>
          <td>
            <b><a href="https://doodle.com/poll/qmbfnp8e2pmr5dw8
">Sign-up</a> for presentations!</b>
          </td>-->
        </tr>
        <tr class='light'>
          <td>Jan 29</td>
          <td>Human-Centered Design<br><br> <i>Lecture</i></td>
          <td>
            <p><span class='optional'>Optional</span></p>
            <p>Rogers, Sharp, and Preece. <a href="https://www.wiley.com/en-us/Interaction+Design%3A+Beyond+Human+Computer+Interaction%2C+6th+Edition-p-9781119901099">Interaction Design: Beyond Human-Computer Interaction</a>. 2015</p>
            <p>Norman. <a href="https://mitpress.mit.edu/9780262525671/the-design-of-everyday-things/">The Design of Everyday Things: Revised and Expanded Edition</a>. 2013</p>
            <p>Martin. <a href="https://www.researchgate.net/profile/Mohammad-Khakshour/publication/283284170_doing_psychology_experiments/links/5630738d08aefac54d8f1cc7/doing-psychology-experiments.pdf">Doing Psychology Experiments</a>. 2007</p>
          </td>
          <td><b>Assignment</b>: Release</td>
          <!--<td>Requesters in crowdsourcing markets <br><br> <i>Lecture</i> (<a href="files/lecture4_0829.pdf">slides</a>)</td>
          <td>
            <p><b>Creating a HIT on MTurk</b><br>
            No required readings. Bring laptops to class.<br>
            <a href="files/RequesterManualPart1.pdf">Be an MTurk Requester (Part 1)</a><br>
            <a href="files/RequesterManualPart2.pdf">Be an MTurk Requester (Part 2)</a>
            </p>
            
          </td>
          <td><b><a href="files/Assignment2.pdf">Assignment 2</a></b>: Due on 11:59pm, Sep 14<br><br>
          </td>-->
        </tr>
         <tr>
          <td>Feb 5</td>
          <td>Design Principles and Guidelines for Human-AI Interaction<br><br> <i>Lecture</i></td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Amershi et al. <a href="https://dl.acm.org/doi/10.1145/3290605.3300233">Guidelines for Human-AI Interaction</a>. CHI'19</p>
            <p><span class='optional'>Optional</span></p>
            <p>Horvitz. <a href="http://erichorvitz.com/chi99horvitz.pdf">Principles of Mixed-Initiative User Interfaces</a>. CHI'99</p>
            <p>Shneiderman. <a href="https://global.oup.com/academic/product/human-centered-ai-9780192845290?cc=de&lang=en&">Human-Centered AI</a>. Oxford University Press, 2022</p>
            <p>Rusell. <a href="https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/">Human Compatible: Artificial Intelligence and the Problem of Control</a>. Penguin Random House, 2019</p>
          </td>
          <td></td>
        </tr>
        <tr class='light'>
          <td>Feb 12</td>
          <td>Explainable AI: Definitions, Methods, and Human-Centered Evaluations <br><br> <!--(<a href="files/lecture5_0905.pdf">slides</a>)--></td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Ribeiro et al. <a href="https://dl.acm.org/doi/10.1145/2939672.2939778">"Why Should I Trust You?": Explaining the Predictions of Any Classifier</a>. KDD'16</p>
            <p>Wang and Yin. <a href="https://dl.acm.org/doi/full/10.1145/3519266">Effects of Explanations in AI-Assisted Decision Making: Principles and Comparisons</a>. ACM Transactions on Interactive Intelligent Systems, 2022</p>
            <p>Cheng et al. <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300789?casa_token=n6VNd7E2r80AAAAA:VvDxcuMGQ_1lTbBEBXdWVI1lLOaVnNNGejE4gigoQ11lxl1jxOih1zNgO8W5nfGmiOVjjzIdml6BU-A">Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders</a>. CHI'19</p>
            <p><span class='optional'>Optional</span></p>
            <p>Lakkaraju et al. <a href="https://dl.acm.org/doi/abs/10.1145/2939672.2939874">Interpretable Decision Sets: A Joint Framework for Description and Prediction</a>. KDD'16</p>
            <p>Miller. <a href="https://www.sciencedirect.com/science/article/pii/S0004370218305988">Explanation in artificial intelligence: Insights from the social sciences</a>. Artificial Intelligence, 2019</p>
            <p>Guidotti et al. <a href="https://dl.acm.org/doi/abs/10.1145/3236009">A Survey of Methods for Explaining Black Box Models</a>. ACM Computing Surveys, August 2018</p>
            <p>Molnar. <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</a>. 2020</p>
            <p>Liao et al. <a href="https://dl.acm.org/doi/10.1145/3313831.3376590">Questioning the AI: Informing Design Practices for Explainable AI User Experiences</a>. CHI'20</p>
            <p>Yang et al. <a href="https://dl.acm.org/doi/abs/10.1145/3377325.3377480?casa_token=Js-UAXfupN4AAAAA:dMmmFbKabVwP5JH_19dHWVXpyVKNRoJgdyIxT3K2ThINgGwAUe5rCmz0jGUhagzihiFPiGxBrGehII4">How Do Visual Explanations Foster End Users' Appropriate Trust in Machine Learning?</a>. IUI'20</p>
            <p>Bucinca et al. <a href="https://dl.acm.org/doi/abs/10.1145/3377325.3377498">Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems</a>. IUI'20</p>
            <p>Poursabzi-Sangdeh et al. <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445315?casa_token=UAQuxSXYty8AAAAA:gD7lcil5zqAQN8CBGE4KFh9CBaoR7c0kfpEb1ZsL54qnigccDMnaNs5ZB990Lw65Sn_dV6_P2hiRXGI">Manipulating and Measuring Model Interpretability</a>. CHI'21</p>
            <p>Bansal et al. <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445717">Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance</a>. CHI'21</p>
            <p>Vasconcelos et al. <a href="https://dl.acm.org/doi/abs/10.1145/3579605?casa_token=H0yuqe--VoYAAAAA:1MF7dYzaY24HU9VVqo_DUJw2cm6M516vR2UkcdCormXs3lo6Uv09DV5QEKDkMeHLQMePUNbl8NchuN8">Explanations Can Reduce Overreliance on AI Systems During Decision-Making</a>. CSCW'23</p>
            <p>Chen et al. <a href="https://dl.acm.org/doi/abs/10.1145/3593013.3593970">Machine Explanations and Human Understanding</a>. FAccT'23</p>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Feb 19</td>
          <td>Explainable AI: Intervention Designs</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Abdul et al. <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376615">COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations</a>. CHI'20</p>
            <p>Lai et al. <a href="https://dl.acm.org/doi/10.1145/3610206">Selective Explanations: Leveraging Human Input to Align Explainable AI</a>. CSCW'23</p>
            <p><span class='optional'>Optional</span></p> 
            <p>Wang et al. <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300831?casa_token=01IdCAXTD0QAAAAA:vo33XKaP-NJNgqedt2E8cHgQEUqy4rkQlgXTZFiYlFTZpnepn5F_puJ0Pl_L70zH8DT3V9CHhzPqbRE">Designing Theory-Driven User-Centric Explainable AI</a>. CHI'19</p>
            <p>Ehsan et al. <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445188">Expanding Explainability: Towards Social Transparency in AI systems</a>. CHI'21</p>
            <p>Fel et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/3d681cc4487b97c08e5aa67224dd74f2-Abstract-Conference.html">Harmonizing the object recognition strategies of deep neural networks with humans</a>. NeurIPS'22</p>
            <p>Nguyen et al. <a href="https://arxiv.org/abs/2208.00780">Visual correspondence-based explanations improve AI robustness and human-AI team accuracy</a>. NeurIPS'22</p>
            <p>Zhang and Lim. <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501826">Towards Relatable Explainable AI with the Perceptual Process</a>. CHI'22</p>
             <p>Gajos and Mamykina <a href="https://dl.acm.org/doi/abs/10.1145/3490099.3511138">Do People Engage Cognitively with AI? Impact of AI Assistance on Incidental Learning</a>. IUI'22</p>
             <p>Danry et al. <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3580672">Don't Just Tell Me, Ask Me: AI Systems that Intelligently Frame Explanations as Questions Improve Human Logical Discernment Accuracy over Causal AI explanations</a>. CHI'23</p>
             <p>Slack et al. <a href="https://dl.acm.org/doi/abs/10.1145/3375627.3375830">Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods</a>. AIES'20</p>
             <p>Slack et al. <a href="https://www.nature.com/articles/s42256-023-00692-8">Explaining machine learning models with interactive natural language conversations using TalkToModel</a>. Nature Machine Intelligence, 2023</p>
             <p>Miller. <a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594001?casa_token=0BzO5z_1kYYAAAAA:dylZ8BCPMm4Ob71EyewGbtZ_5fWUrnwQrdUJNOoPhG7T5RK_qvGtWHCF99cDsUlZnLr9gBaaoGu1e-w">Explainable AI is Dead, Long Live Explainable AI!: Hypothesis-driven Decision Support using Evaluative AI</a>. FAccT'23</p>
          </td>
          <td><b>Project</b>: Proposal due (by EOD, Feb 21)</td>
        </tr>
         <tr class='light'>
          <td>Feb 26</td>
          <td>Trust and Reliance on AI: Empirical Studies and Computational Models</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Rechkemmer and Yin. <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501967">When Confidence Meets Accuracy: Exploring the Effects of Multiple Performance Indicators on Trust in Machine Learning Models</a>. CHI'22</p>
            <p>Tejeda et al. <a href="https://link.springer.com/article/10.1007/s42113-022-00157-y">AI-Assisted Decision-making: a Cognitive Modeling Approach to Infer Latent Reliance Strategies</a>. Computational Brain & Behavior, 2022</p>
            <p><span class='optional'>Optional</span></p>
            <p>Bansal et al. <a href="https://ojs.aaai.org/index.php/HCOMP/article/view/5285">Beyond Accuracy: The Role of Mental Models in Human-AI Team Performance</a>. HCOMP'19</p>
            <p>Bansal et al. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4087">Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff</a>. AAAI'19</p>
            <p>Zhang et al. <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372852?casa_token=KHaCi589vsAAAAAA:QxPnT7_Bx222zMo3sCknnoALQs8krAknbDQyLrWNInQH9nvS7uJIcqjOvisJD_dgavPUMgNCQDzr7w8">Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making</a>. FAT*'20</p>
            <p>Nourani et al. <a href="https://ojs.aaai.org/index.php/HCOMP/article/view/7469">The Role of Domain Expertise in User Trust and the Impact of First Impressions with Intelligent Systems</a>. HCOMP'20</p>
            <p>Lu and Yin. <a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445562">Human Reliance on Machine Learning Models When Performance Feedback is Limited: Heuristics and Risks</a>. CHI'21</p>
            <p>Guo and Yang. <a href="https://link.springer.com/article/10.1007/s12369-020-00703-3">Modeling and Predicting Trust Dynamics in Human-Robot Teaming: A Bayesian Inference Approach</a>. International Journal of Social Robotics, 2021</p>
            <p>Azevedo-Sa et al. <a href="https://link.springer.com/article/10.1007/s12369-020-00694-1">Real-Time Estimation of Drivers' Trust in Automated Driving Systems</a>. International Journal of Social Robotics, 2021</p>
            <p>Wang et al. <a href="https://dl.acm.org/doi/abs/10.1145/3485447.3512240">Will You Accept the AI Recommendation? Predicting Human Behavior in AI-Assisted Decision Making</a>. WWW'22</p>
            <p>Papenmeier et al. <a href="https://dl.acm.org/doi/full/10.1145/3495013">It's Complicated: The Relationship between User Trust, Model Accuracy and Explanations in AI</a>. ACM Transactions on Computer-Human Interaction, 2022</p>
            <p>Chong et al. <a href="https://www.sciencedirect.com/science/article/pii/S0747563221003411">Human confidence in artificial intelligence and in themselves: The evolution and impact of confidence on adoption of AI advice</a>. Computers in Human Behavior, 2022</p>
            <p>Li et al. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25748">Modeling Human Trust and Reliance in AI-Assisted Decision Making: A Markovian Approach</a>. AAAI'23</p>
            <p>Chen et al. <a href="https://dl.acm.org/doi/abs/10.1145/3610219">Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations</a>. CSCW'23</p>
          </td>
          <td>
            <b>Assignment</b>: Due by EOD, Feb 26
          </td>
        </tr>
        <tr>
          <td>Mar 4</td>
          <td>Trust and Reliance on AI: Intervention Designs <!--(<a href="files/lecture5_0905.pdf">slides</a>)--></td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Ma et al. <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581058">Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making</a>. CHI'23</p>
            <p>Bansal et al. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17359">Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork</a>. AAAI'21</p>
             <p><span class='optional'>Optional</span></p>
             <p>Bucinca et al. <a href="https://dl.acm.org/doi/abs/10.1145/3449287">To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making</a>. CSCW'21</p>
             <p>Rastogi et al. <a href="https://dl.acm.org/doi/abs/10.1145/3512930">Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making</a>. CSCW'22</p>
             <p>Vodrahalli et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/1968ea7d985aa377e3a610b05fc79be0-Abstract-Conference.html">Uncalibrated Models Can Improve Human-AI Collaboration</a>. NeurIPS'22</p>
             <p>Benz and Rodriguez. <a href="https://arxiv.org/abs/2306.00074">Human-Aligned Calibration for AI-Assisted Decision Making</a>. NeurIPS'23</p>
             <p>Cabrera et al. <a href="https://dl.acm.org/doi/10.1145/3579612">Improving Human-AI Collaboration With Descriptions of AI Behavior</a>. CSCW'23</p>
             <p>Noti and Chen. <a href="https://www.ijcai.org/proceedings/2023/0339.pdf">Learning When to Advise Human Decision Makers</a>. IJCAI'23</p>
             <p>Li et al. <a href="https://www.ijcai.org/proceedings/2023/0337.pdf">Strategic Adversarial Attacks in AI-assisted Decision Making to Reduce Human Trust and Reliance</a>. IJCAI'23</p>
             <p>Inkpen et al. <a href="https://dl.acm.org/doi/abs/10.1145/3534561?casa_token=jjRe0BkaM8cAAAAA%3Al-x_piEAvGF7aIzn7bEpkS0GiJqwbiFsqs00gDKEFiN-uHdVy-wjmtvb4gv2-rPkgHr2Ld08LiL4R8g">Advancing Human-AI Complementarity: The Impact of User Expertise and Algorithmic Tuning on Joint Decision Making</a>. ACM Transactions on Computer-Human Interaction, 2023</p>
          <td></td>
        </tr>
        <tr class='light'>
          <td>Mar 11</td>
          <td class='dark'><i>No class (Spring break)</i></td>
          <td>
          </td>
          <td></td>
          <!--<td>Crowdsourcing:<br> Opportunities and challenges <br><br> <i>Lecture</i> (<a href="files/lecture6_0910.pdf">slides</a>)</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Kittur et al. <a href="https://dl.acm.org/citation.cfm?id=2441923">The Future of Crowd Work</a>. CSCW'13</p>
            <p><span class='optional'>Optional</span></p>
            <p>Quinn and Bederson. <a href="https://dl.acm.org/citation.cfm?id=1979148">Human Computation: A Survey and Taxonomy of a Growing Field</a>. CHI'11</p>
            <p>Gadiraju et al. <a href="http://ieeexplore.ieee.org/abstract/document/7156008/?anchor=authors">Human Beyond the Machine: Challenges and Opportunities of Microtask Crowdsourcing</a>. IEEE Intelligent Systems, July 2015</p>
          </td>
          <td></td>-->
        </tr>
        <tr>
          <td>Mar 18</td>
          <td>Bias and Fairness in AI: Definitions and Methods</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Angwin et al. <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias</a>. Propublica, 2016</p>
             <p>Srinivasan and Chander. <a href="https://dl.acm.org/doi/abs/10.1145/3466132.3466134">Biases in AI Systems: A survey for practitioners</a>. Communications of ACM, 2021</p>
             <p>Zafar et al. <a href="https://dl.acm.org/doi/abs/10.1145/3038912.3052660?casa_token=Jg5fOSDl5HYAAAAA:INDRVrUWiRVUnMn6jcQS8tK2pSOOGyWX9Ogv5BHtwDCq10Rt6sqV3FNg4pLaXqumfVXgnnZ8eiwT83Y">Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment</a>. WWW'17</p>
            <p><span class='optional'>Optional</span></p>
             <p>Hardt et al. <a href="https://proceedings.neurips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html">Equality of Opportunity in Supervised Learning</a>. NeurIPS'16</p>
            <p>Caliskan et al. <a href="https://www.science.org/doi/full/10.1126/science.aal4230?casa_token=IHnmQiJcdQEAAAAA%3Ab-oBWuqhk-DQ_mn8HKm70AYRnjIzXo0nQ5f36j4hDsoMHTDSoauF0JYnOli4ELhJAc4BwwgXA3lmgqLJ">Semantics derived automatically from language corpora contain human-like biases</a>. Science, 2017</p>
            <p>Kusner et al. <a href="https://proceedings.neurips.cc/paper/2017/hash/a486cd07e4ac3d270571622f4f316ec5-Abstract.html">Counterfactual Fairness</a>. NeurIPS'17</p>
            <p>Otterbacher et al. <a href="https://dl.acm.org/doi/abs/10.1145/3025453.3025727">Competent Men and Warm Women: Gender Stereotypes and Backlash in Image Search Results</a>. CHI'17</p>
            <p>Hube et al. <a href="https://dl.acm.org/doi/abs/10.1145/3290605.3300637?casa_token=0qVLMvSco_sAAAAA:wB6yyQEi4g7IpYK9AM4Jrb6plJH0kr5c-LgMGPtVfyX9ZHLld5M78Sdi9iSr9G38ktARKfmTq1DYEyY">Understanding and Mitigating Worker Biases in the Crowdsourced Collection of Subjective Judgments</a>. CHI'19</p>
            <p>Bellamy et al. <a href="https://ieeexplore.ieee.org/abstract/document/8843908">AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias</a>. IBM Journal of Research and Development, 2019</p>
            <p>Karimi et al. <a href="https://dl.acm.org/doi/abs/10.1145/3442188.3445899?casa_token=wVpl0rfCgxgAAAAA:WqQmW_jxjQP2OtknP4hA-QI7XVQF9a_KucSKQnlAhz34pBEMGHh09TEwsDJrOqBWoNos3P4THc4szr8">Algorithmic Recourse: from Counterfactual Explanations to Interventions</a>. FAccT'21</p>
            <p>Mehrabi et al. <a href="https://dl.acm.org/doi/abs/10.1145/3457607?casa_token=triVrb5PpiEAAAAA:ZzLFVPL9nKnzjivJD3RWcKekkeiypN0158Fc4z_O30MbG0IGRLI6ElWRAJMbgu_OodGmuBRZAAr3n2I">A Survey on Bias and Fairness in Machine Learning</a>. ACM Computing Surveys, 2021</p>
            <p>Mitchell et al. <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-042720-125902">Algorithmic Fairness: Choices, Assumptions, and Definitions</a>. Annual Review of Statistics and Its Application, 2021</p>
            <p>Sap et al. <a href="https://aclanthology.org/2022.naacl-main.431/">AAnnotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection</a>. NAACL'22</p>
          </td>
          <td></td>
        </tr>
        <tr class='light'>
          <td>Mar 25</td>
          <td>Bias and Fairness in AI: Empirical Studies and Intervention Designs</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Wang et al. <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376813">Factors Influencing Perceived Fairness in Algorithmic Decision-Making: Algorithm Outcomes, Development Procedures, and Individual Differences</a>. CHI'20</p>
            <p>Green and Chen. <a href="https://dl.acm.org/doi/abs/10.1145/3287560.3287563">Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments</a>. FAT*'19</p>
            <p><span class='optional'>Optional</span></p>
            <p>Liu et al. <a href="https://proceedings.mlr.press/v80/liu18c.html">Delayed Impact of Fair Machine Learning</a>. ICML'18</p>
            <p>Grgic-Hlaca et al. <a href="https://dl.acm.org/doi/abs/10.1145/3178876.3186138">Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction</a>. WWW'18</p>
            <p>Srivastava et al. <a href="https://dl.acm.org/doi/abs/10.1145/3292500.3330664">Mathematical Notions vs. Human Perception of Fairness: A Descriptive Approach to Fairness for Machine Learning</a>. KDD'19</p>
            <p>Saxena et al. <a href="https://dl.acm.org/doi/abs/10.1145/3306618.3314248?casa_token=KzoD76yrXDgAAAAA:0XEKDM3dJjLhxMZ0ydzAI6mX4MCMJQXULBf_-wkDbYsBR1Kt_vqSl5Sf5GtA0QWGfpwlqzqgdOgggdU">How Do Fairness Definitions Fare?: Examining Public Attitudes Towards Algorithmic Definitions of Fairness</a>. AIES'19</p>
            <p>Dodge et al. <a href="https://dl.acm.org/doi/abs/10.1145/3301275.3302310">Explaining models: An empirical study of how explanations impact fairness judgment</a>. IUI'19</p>
            <p>Zhang et al. <a href="https://proceedings.neurips.cc/paper_files/paper/2019/hash/7690dd4db7a92524c684e3191919eb6b-Abstract.html">Group Retention when Using Machine Learning in Sequential Decision Making: the Interplay between User Dynamics and Fairness</a>. NeurIPS'19</p>
            <p>Zhang et al. <a href="https://proceedings.neurips.cc/paper/2020/hash/d6d231705f96d5a35aeb3a76402e49a3-Abstract.html">How do fair decisions fare in long-term qualification?</a>. NeurIPS'20</p>
            <p>Gemalmaz and Yin. <a href="https://www.ijcai.org/proceedings/2021/0238.pdf">Accounting for Confirmation Bias in Crowdsourced Label Aggregation</a>. IJCAI'21</p>
            <p>Cheng et al. <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501831">How Child Welfare Workers Reduce Racial Disparities in Algorithmic Decisions</a>. CHI'22</p>
            <p>Gordon et al. <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3502004">Jury Learning: Integrating Dissenting Voices into Machine Learning Models</a>. CHI'22</p>
            <p>Wang et al. <a href="https://www.ijcai.org/proceedings/2023/0343.pdf">The Effects of AI Biases and Explanations on Human Decision Fairness: A Case Study of Bidding in Rental Housing Markets</a>. IJCAI'23</p>
          </td>
          <td><b>Project</b>: Midterm report due (by EOD, Mar 27)</td>
        </tr>
        <tr>
          <td>Apr 1</td>
          <td>Human-AI Collaboration and Teaming</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Lai et al. <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501999">Human-AI Collaboration via Conditional Delegation: A Case Study of Content Moderation</a>. CHI'22</p>
            <p>Hong et al. <a href="https://arxiv.org/abs/2303.02265">Learning to Influence Human Behavior with Offline Reinforcement Learning</a>. NeurIPS'23</p>
            <p><span class='optional'>Optional</span></p>
            <p>Madras et al. <a href="https://proceedings.neurips.cc/paper/2018/hash/09d37c08f7b129e96277388757530c72-Abstract.html">Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer</a>. NeurIPS'18</p>
            <p>Nushi et al. <a href="https://ojs.aaai.org/index.php/HCOMP/article/view/13337">Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure</a>. HCOMP'18</p>
            <p>Carroll et al. <a href="https://proceedings.neurips.cc/paper/2019/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html">On the Utility of Learning about Humans for Human-AI Coordination</a>. NeurIPS'19</p>
            <p>Chakraborti et al. <a href="https://www.ijcai.org/Proceedings/2019/0185.pdf">Balancing Explicability and Explanations for Human-Aware Planning</a>. IJCAI'19</p>
            <p>Gennatas et al. <a href="https://www.pnas.org/doi/abs/10.1073/pnas.1906831117">
            Expert-augmented machine learning</a>. PNAS, 2020</p>
            <p>Xiao et al. <a href="https://dl.acm.org/doi/abs/10.5555/3398761.3398935">
            FRESH: Interactive Reward Shaping in High-Dimensional State Spaces using Human Feedback</a>. AAMAS'20</p>
             <p>Wilder et al. <a href="https://www.ijcai.org/Proceedings/2020/0212.pdf">
            Learning to Complement Humans</a>. IJCAI'20</p>
            <p>Gao et al. <a href="https://www.ijcai.org/proceedings/2021/0237.pdf">
            Human-AI Collaboration with Bandit Feedback</a>. IJCAI'21</p>
            <p>Steyvers et al. <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2111547119">
            Bayesian modeling of human-AI complementarity</a>. PNAS, 2022</p>
            <p>Callaway et al. <a href="https://www.pnas.org/doi/abs/10.1073/pnas.2117432119">
            Leveraging artificial intelligence to improve people's planning strategies</a>. PNAS, 2022</p>
            <p>Schelble et al. <a href="https://dl.acm.org/doi/abs/10.1145/3492832?casa_token=lgqFGY5y81wAAAAA:X2UAtIVbic_RVgk07L1MWZwzliFWP2a5X5HuCiSMF5OzUEGEIGoDNIiFmtMflC6xsbwcnvqBpnOlB6w">
            Let's Think Together! Assessing Shared Mental Models, Performance, and Trust in Human-Agent Teams</a>. GROUP'22</p>
          </td>
          <td></td>
        </tr>
        <tr class='light'>
          <td>Apr 8</td>
          <td>Human Interaction with Large Language Models</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Zamfirescu-Pereira et al. <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581388">Why Johnny Can't Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts</a>. CHI'23</p>
            <p>Jakesch et al. <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3581196">Co-Writing with Opinionated Language Models Affects Users' Views</a>. CHI'23</p>
             <p><span class='optional'>Optional</span></p>
              <p>Wu et al. <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3517582">AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts</a>. CHI'22</p>
              <p>Yuan et al. <a href="https://dl.acm.org/doi/abs/10.1145/3490099.3511105">Wordcraft: Story Writing With Large Language Models</a>. IUI'22</p>
            <p>Noy and Zhang. <a href="https://www.science.org/doi/10.1126/science.adh2586">Experimental evidence on the productivity effects of generative artificial intelligence</a>. Science, 2023</p>
             <p>Argyle et al. <a href="https://www.pnas.org/doi/10.1073/pnas.2311627120">Leveraging AI for democratic discourse: Chat interventions can improve online political conversations at scale</a>. PNAS, 2023</p>
             <p>Wang et al. <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3580948">PopBlends: Strategies for Conceptual Blending with Large Language Models</a>. CHI'23</p>
             <p>Chung et al. <a href="https://arxiv.org/abs/2306.04140">Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions</a>. ACL'23</p>
             <p>Fok et al. <a href="https://dl.acm.org/doi/abs/10.1145/3581641.3584034">Scim: Intelligent Skimming Support for Scientific Papers</a>. IUI'23</p>
             <p>Rastogi et al. <a href="https://dl.acm.org/doi/abs/10.1145/3600211.3604712">Supporting Human-AI Collaboration in Auditing LLMs with LLMs</a>. AIES'23</p>
             <p>Xiao et al. <a href="https://dl.acm.org/doi/abs/10.1145/3581754.3584136">Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding</a>. IUI'23 Companion</p>
          </td>
          <td></td>
        </tr>
        <tr>
          <td>Apr 15</td>
          <td>AI, Ethics, and Society</td>
          <td>
            <p><span class='required'>Required</span></p>
            <p>Awad et al. <a href="https://www.nature.com/articles/s41586-018-0637-6.">The Moral Machine experiment</a>. Nature, 2018</p>
            <p>Gabriel. <a href="https://link.springer.com/article/10.1007/s11023-020-09539-2">Artificial Intelligence, Values, and Alignment</a>. Minds and Machines, 2020</p>
            <p><span class='optional'>Optional</span></p>
            <p>Conitzer et al. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/11140">Moral Decision Making Frameworks for Artificial Intelligence</a>. AAAI'17</p>
            <p>Zhu et al. <a href="https://dl.acm.org/doi/abs/10.1145/3274463">Value-Sensitive Algorithm Design: Method, Case Study, and Lessons</a>. CSCW'18</p>
            <p>Smith et al. <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376783">Keeping Community in the Loop: Understanding Wikipedia Stakeholder Values for Machine Learning-Based Systems</a>. CHI'20</p>
            <p>Tolmeijer et al. <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3517732">Capable but Amoral? Comparing AI and Human Expert Collaboration in Ethical Decision Making</a>. CHI'22</p>
            <p>Zhang et al. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0022103122000464">Artificial intelligence and moral dilemmas: Perception of ethical decision-making in AI</a>. Journal of Experimental Social Psychology, 2022</p>
            <p>Weidinger et al. <a href="https://www.pnas.org/doi/10.1073/pnas.2213709120">Using the Veil of Ignorance to align AI systems with principles of justice</a>. PNAS, 2023</p>
            <p>Narayanan et al. <a href="https://dl.acm.org/doi/abs/10.1145/3600211.3604709">How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?</a>. AIES'23</p>
            <p>Rezwana and Maher. <a href="https://dl.acm.org/doi/abs/10.1145/3591196.3593364">User Perspectives on Ethical Challenges in Human-AI Co-Creativity: A Design Fiction Study</a>. C&C'23</p>
          </td>
          <td></td>
        </tr>
         <tr class='light'>
          <td>Apr 22</td>
         <td class='project'><b>Final project presentation</b></td>
          <td></td>
          <td><b>Project</b>: Final project due (by EOD, Apr 28)</td>
          
        </tr>
      </table>
    </div>
  	<!--<div class="box">
      <h3>Week 1 (Jan 8)</h3>
      <b>Knowing by Asking I: Paper Critique</b>
      <br>Lecture: <a href="lectures/Jan8/Overview.pdf">Overview</a>, <a href="lectures/Jan8/Asking.pdf">Knowing By Asking</a>
      <br>
      <br>Required Reading:
      <br><a href="reading/Asking/Paper1.pdf">Ray and Mondada, What do People Expect from Robots? In IROS, 2008.</a>
      <br><a href="reading/Asking/Paper2.pdf">Luger and Sellen, "Like Having a Really bad PA": The Gulf between User Expectation and Experience of A Conversational Agent.  In CHI, 2016</a>
      <br><a href="reading/Asking/Paper3.pdf">Bryant et al., Becoming Wikipedian: Transformation of Participation in a Collaborative Online Encyclopedia.  In Group, 2005.</a>
      <br>
    </div>-->
  	
</div>

<!-- ####################################################################################################### -->
<div class="wrapper col6">
  <div id="copyright">
    <p class="fl_right">Design by <a href="http://www.os-templates.com/" title="Free Website Templates">OS Templates</a></p>
    <br class="clear" />
  </div>
</div>
<!-- ####################################################################################################### -->
<script type="text/javascript" src="scripts/toggle.js"></script>
</body>
</html>
